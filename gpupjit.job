#!/bin/bash -x
#SBATCH --job-name=JAXHPC
#SBATCH --partition=gpu_a100_il
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1               # ‚úÖ Only 1 task per node
#SBATCH --gpus-per-node=4                 # ‚úÖ Use all 4 GPUs in a single process
#SBATCH --time=00:30:00
#SBATCH --mem=16G
#SBATCH --export=ALL

set -o pipefail
export PYTHONUNBUFFERED=1

# ‚úÖ Load modules
module load devel/python/3.11.7-gnu-14.2
module load devel/cuda/12.8

ENV_DIR=$HOME/pjit_jax_env

# ‚úÖ Setup environment
if [ ! -d "$ENV_DIR" ]; then
    python -m venv $ENV_DIR
    source $ENV_DIR/bin/activate
    pip install --upgrade pip
    pip install --upgrade "jax[cuda12]"  # Installs jaxlib with CUDA support
    pip install numpy matplotlib
else
    source $ENV_DIR/bin/activate
fi

# ‚úÖ Optional: prevent full GPU memory preallocation
export XLA_PYTHON_CLIENT_PREALLOCATE=false

# ‚úÖ Fix MPI path issues (even though unused here)
unset LD_LIBRARY_PATH

# ‚úÖ GPU check
nvidia-smi

# ‚úÖ Unset CUDA_VISIBLE_DEVICES to expose all GPUs to the task
unset CUDA_VISIBLE_DEVICES

python -c "import jax; print('JAX devices:', jax.devices())"

# ‚úÖ Determine coordinator hostname (first node in allocation)
COORD_HOST=$(scontrol show hostnames $SLURM_NODELIST | head -n 1)

# ‚úÖ Launch 1 JAX process per node via `srun`
# ======= ‚úÖ Highlighted Fix: Ensure each task sees all 4 GPUs =======
srun --ntasks=$SLURM_NNODES \
     --nodes=$SLURM_NNODES \
     --gpus-per-task=4 \
     --export=ALL,CUDA_VISIBLE_DEVICES \
     bash -c '
export PYTHONUNBUFFERED=1
export JAX_COORDINATOR='"$COORD_HOST"':1234
export JAX_NUM_PROCESSES=$SLURM_NNODES
export JAX_PROCESS_ID=$SLURM_PROCID

echo "üîÅ Starting JAX process $JAX_PROCESS_ID on $(hostname)..."
python JD_PJIT.py
'
